{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dump_small_clean.jsonln', 'r', encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickLowercaseWords(tokens):\n",
    "    return [token.lower() for token in tokens if re.fullmatch('\\w+', token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 11225/11225 [00:12<00:00, 887.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_words = []\n",
    "for item in tqdm(data):\n",
    "    texto = item['body']\n",
    "    tokens = word_tokenize(texto)\n",
    "    tokens = pickLowercaseWords(tokens)\n",
    "    all_words += tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942514"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(word_list):\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    stopwords += nltk.corpus.stopwords.words('english')\n",
    "    return [i for i in word_list if i not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(removeStopwords(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_list = list(word_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_list_sorted = sorted(word_counts_list, key=lambda x: (-x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_counts_list_sorted[:10000] #10000 mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(vocab) #Vocabulario de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWERCASE = [chr(x) for x in range(ord('a'), ord('z') + 1)]\n",
    "LOWERCASE_OTHERS = [chr(x) for x in range(129, 164)] # ü até ú https://theasciicode.com.ar/\n",
    "LETTERS = LOWERCASE + LOWERCASE_OTHERS\n",
    "\n",
    "def edit1(text):\n",
    "    words = []\n",
    "    \n",
    "    # Fase 1: as remoçoes.\n",
    "    for p in range(len(text)):\n",
    "        new_word = text[:p] + text[p + 1:]\n",
    "        if len(new_word) > 0:\n",
    "            words.append(new_word)\n",
    "        \n",
    "    # Fase 2: as adições.\n",
    "    for p in range(len(text) + 1):\n",
    "        for c in LETTERS:\n",
    "            new_word = text[:p] + c + text[p:]\n",
    "            words.append(new_word)\n",
    "    \n",
    "    # Fase 3: as substituições.\n",
    "    for p in range(len(text)):\n",
    "        orig_c = text[p]\n",
    "        for c in LETTERS:\n",
    "            if orig_c != c:\n",
    "                new_word = text[:p] + c + text[p + 1:]\n",
    "                words.append(new_word)\n",
    "    \n",
    "    return set(words)\n",
    "\n",
    "def edit2(text):\n",
    "    words1 = edit1(text)\n",
    "    words2 = set()\n",
    "    for w in words1:\n",
    "        candidate_words2 = edit1(w)\n",
    "        candidate_words2 -= words1\n",
    "        words2.update(candidate_words2)\n",
    "    words2 -= set([text])\n",
    "    return words2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truque do norvig\n",
    "# def candidates(word):\n",
    "#     candidatos = \\\n",
    "#         set([i for i in [word] if i in vocab]) or \\\n",
    "#         set([w for w in edit1(word) if w in vocab]) or \\\n",
    "#         set([w for w in edit2(word) if w in vocab]) or\\\n",
    "#         set([word])\n",
    "#     return candidatos\n",
    "\n",
    "# truque do norvig \"traduzido\"\n",
    "def candidates(word):\n",
    "    editD1 = [w for w in edit1(word) if w in vocab]\n",
    "    editD2 = [w for w in edit2(word) if w in vocab]\n",
    "    if word in vocab:\n",
    "        return [word]\n",
    "    elif editD1:\n",
    "        return editD1\n",
    "    elif editD2:\n",
    "        return editD2\n",
    "    else:\n",
    "        return [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 1e5\n",
    "def P(word, N=sum(vocab.values())):\n",
    "    count = vocab[word] if word in vocab else 0\n",
    "    return (count+1) / (N+V) # Perguntar tecnica de smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cavalo'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correction(word):\n",
    "    # candidates_ = candidates(word)\n",
    "    # best, prob = None, 0\n",
    "    # for i in candidates_:\n",
    "    #     p = P(i)\n",
    "    #     if p > prob:\n",
    "    #         prob = p\n",
    "    #         best = i\n",
    "    # return best\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "correction('cavako')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.fullmatch(r'\\w+', 'Variações Varia\\u00e7\\u00f5es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o mateus , e o tiago 1434 é eficiente 123'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenCorrection(text):\n",
    "    tokenizer = RegexpTokenizer(\"(?:[\\w']+)|(?:[,.;!?:])\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    corrected_tokens = []\n",
    "    for t in tokens:\n",
    "        if t in \",.;!?:\":\n",
    "            corrected_tokens += [t]\n",
    "        elif t.isdigit():\n",
    "            corrected_tokens += [t]\n",
    "        else:\n",
    "            if t in nltk.corpus.stopwords.words('portuguese'):\n",
    "                corrected_tokens += [t]\n",
    "            else:\n",
    "                corrected_tokens += [correction(t)]\n",
    "\n",
    "    return \" \".join(corrected_tokens)\n",
    "\n",
    "tokenCorrection(\"o matheus, e o thiago 1434 é efiiente 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuationCorrection(text):\n",
    "    text = re.sub(r\"\\s([,.;!?:](?:\\s|$))\", r\"\\1\", text)\n",
    "    return re.sub(r\"(^|[.?!])\\s*(\\w)\", lambda p: p.group(0).upper(), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textCorrection(text):\n",
    "    funcs = [\n",
    "        tokenCorrection,\n",
    "        punctuationCorrection\n",
    "    ]\n",
    "    \n",
    "    x = text\n",
    "    for func in funcs:\n",
    "        x = func(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O mateus é eficiente',\n",
       " 'O trabalho está bom',\n",
       " 'Bom di, pessoal',\n",
       " 'Rei ao mercado. Quer de algo?',\n",
       " 'O andré é muito banana']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = [\n",
    "    \"o matheus é efiiente\",\n",
    "    \"o trabalo está bom\",\n",
    "    \"bom di, pesoal\",\n",
    "    \"irei ao mercaso. quer de algo?\",\n",
    "    \"o andré é muito bacana\"\n",
    "]\n",
    "\n",
    "res = list(map(textCorrection, strings))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perguntar tecnica de smoothing na probabiliadde da palavra\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
